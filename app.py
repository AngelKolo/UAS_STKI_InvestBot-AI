# -*- coding: utf-8 -*-
"""
InvestBot AI - Streamlit Version with Groq LLM
UAS SISTEM TEMU KEMBALI INFORMASI (STKI) GANJIL 2025/2026

Nama   : Angelica Widyastuti Kolo
NIM    : A11.2021.13212
Kelas  : A11.4706
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import math
import nltk
import os
from typing import List
from collections import Counter
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Try import Groq (optional)
try:
    from groq import Groq
    HAS_GROQ = True
except ImportError:
    HAS_GROQ = False

# ================================================================================
# CONFIGURATION
# ================================================================================
st.set_page_config(
    page_title="InvestBot AI",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #2B6CB0;
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        text-align: center;
        color: #555;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    .result-box {
        background-color: #f0f8ff;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #2B6CB0;
        margin: 1rem 0;
    }
    .sentiment-positive {
        color: #28a745;
        font-weight: bold;
    }
    .sentiment-negative {
        color: #dc3545;
        font-weight: bold;
    }
    .sentiment-neutral {
        color: #ffc107;
        font-weight: bold;
    }
    .footer {
        text-align: center;
        color: #888;
        font-size: 0.9rem;
        margin-top: 3rem;
        padding-top: 1rem;
        border-top: 1px solid #ddd;
    }
</style>
""", unsafe_allow_html=True)

# ================================================================================
# DOWNLOAD NLTK DATA (ONE-TIME)
# ================================================================================
@st.cache_resource
def download_nltk_data():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
        nltk.download('punkt_tab', quiet=True)

download_nltk_data()

# ================================================================================
# TEXT PREPROCESSING
# ================================================================================
class TextPreprocessor:
    def __init__(self):
        factory = StemmerFactory()
        self.stemmer = factory.create_stemmer()
        self.stopwords = set(['yang', 'di', 'ke', 'dari', 'dan', 'ini', 'itu', 
                              'adalah', 'ada', 'untuk', 'pada', 'dengan', 'atau'])

    def preprocess(self, text: str) -> List[str]:
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)
        tokens = text.split()
        clean_tokens = [self.stemmer.stem(t) for t in tokens 
                       if t not in self.stopwords and len(t) > 2]
        return clean_tokens

# ================================================================================
# TF-IDF IMPLEMENTATION
# ================================================================================
class TFIDFFromScratch:
    def __init__(self):
        self.vocabulary = {}
        self.idf = {}
        self.n_docs = 0

    def fit(self, docs_tokens: List[List[str]]):
        self.n_docs = len(docs_tokens)
        df_counts = Counter()
        all_terms = set()
        
        for doc in docs_tokens:
            unique_terms = set(doc)
            all_terms.update(unique_terms)
            for term in unique_terms:
                df_counts[term] += 1

        self.vocabulary = {term: i for i, term in enumerate(sorted(list(all_terms)))}
        for term, df in df_counts.items():
            self.idf[term] = math.log(self.n_docs / (1 + df))

    def transform(self, docs_tokens: List[List[str]]) -> np.ndarray:
        matrix = np.zeros((len(docs_tokens), len(self.vocabulary)))
        for i, doc in enumerate(docs_tokens):
            tf_counts = Counter(doc)
            for term, count in tf_counts.items():
                if term in self.vocabulary:
                    tf = count / len(doc) if len(doc) > 0 else 0
                    matrix[i, self.vocabulary[term]] = tf * self.idf[term]
        return matrix

def cosine_similarity(v1, v2):
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    return dot_product / (norm_v1 * norm_v2) if norm_v1 > 0 and norm_v2 > 0 else 0

# ================================================================================
# SENTIMENT ANALYSIS - K-NN CLASSIFIER
# ================================================================================
class KNNSentimentClassifier:
    """K-NN Classifier untuk sentiment analysis menggunakan dataset"""
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = X
        self.y_train = np.array(y)

    def predict(self, x_query):
        similarities = [cosine_similarity(x_query, x_t) for x_t in self.X_train]
        k_indices = np.argsort(similarities)[-self.k:][::-1]
        k_nearest_labels = self.y_train[k_indices]
        most_common = Counter(k_nearest_labels).most_common(1)[0][0]
        return most_common

class SentimentAnalyzer:
    """Hybrid: ML-based (K-NN) + Lexicon fallback"""
    def __init__(self, use_ml=True):
        self.use_ml = use_ml
        self.knn_model = None
        self.sentiment_tfidf = None
        self.sentiment_preprocessor = None
        
        # Lexicon fallback
        self.pos_words = set(['untung', 'naik', 'bagus', 'cuan', 'aman', 'stabil', 
                             'rekomendasi', 'tumbuh', 'profit', 'menguntung', 
                             'positif', 'baik', 'sukses', 'mantap', 'joss'])
        self.neg_words = set(['rugi', 'turun', 'anjlok', 'bahaya', 'resiko', 'buruk', 
                             'waspada', 'bangkrut', 'loss', 'negatif', 'jelek', 'gagal'])
    
    def train_from_dataset(self, df_sentiment):
        """Train K-NN model dari sentiment_opinions.csv"""
        try:
            self.sentiment_preprocessor = TextPreprocessor()
            self.sentiment_tfidf = TFIDFFromScratch()
            
            # Preprocess sentiment data
            processed_sents = [self.sentiment_preprocessor.preprocess(str(text)) 
                              for text in df_sentiment['text']]
            
            # Fit TF-IDF
            self.sentiment_tfidf.fit(processed_sents)
            X_sentiment = self.sentiment_tfidf.transform(processed_sents)
            
            # Train K-NN
            self.knn_model = KNNSentimentClassifier(k=5)
            self.knn_model.fit(X_sentiment, df_sentiment['label'].values)
            
            return True
        except Exception as e:
            print(f"Warning: Gagal train sentiment model: {e}")
            return False
    
    def analyze(self, text):
        """Analyze sentiment with ML or fallback to lexicon"""
        # Try ML-based first
        if self.use_ml and self.knn_model is not None:
            try:
                tokens = self.sentiment_preprocessor.preprocess(text)
                vec = self.sentiment_tfidf.transform([tokens])[0]
                prediction = self.knn_model.predict(vec)
                
                # Map label ke format output
                label_map = {
                    'positif': ('Positif', 'sentiment-positive'),
                    'negatif': ('Negatif', 'sentiment-negative'),
                    'netral': ('Netral', 'sentiment-neutral')
                }
                return label_map.get(prediction.lower(), ('Netral', 'sentiment-neutral'))
            except:
                pass
        
        # Fallback to lexicon-based
        tokens = text.lower().split()
        score = (sum(1 for t in tokens if t in self.pos_words) - 
                sum(1 for t in tokens if t in self.neg_words))
        
        if score > 0:
            return "Positif", "sentiment-positive"
        elif score < 0:
            return "Negatif", "sentiment-negative"
        else:
            return "Netral", "sentiment-neutral"

# ================================================================================
# LLM ANSWER GENERATOR (GROQ + FALLBACK)
# ================================================================================
class LLMAnswerGenerator:
    """Generate natural answers using Groq LLM with extractive fallback"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.use_llm = False
        self.client = None
        
        # Try to initialize Groq
        if HAS_GROQ:
            try:
                # Try to get API key from Streamlit secrets or environment
                api_key = None
                if hasattr(st, 'secrets') and 'GROQ_API_KEY' in st.secrets:
                    api_key = st.secrets['GROQ_API_KEY']
                elif 'GROQ_API_KEY' in os.environ:
                    api_key = os.environ['GROQ_API_KEY']
                
                if api_key:
                    self.client = Groq(api_key=api_key)
                    self.use_llm = True
                    print("Groq LLM initialized successfully!")
            except Exception as e:
                print(f"Groq initialization failed: {e}")
    
    def generate_answer(self, context: str, query: str, category: str) -> str:
        # Try LLM first if available
        if self.use_llm and self.client:
            try:
                prompt = f"""Kamu adalah asisten investasi yang membantu edukasi pengguna. Berdasarkan konteks berikut, jawab pertanyaan dengan singkat, jelas, dan informatif (maksimal 4 kalimat) dalam Bahasa Indonesia.

Konteks: {context[:1500]}

Pertanyaan: {query}

Jawaban (singkat dan jelas):"""
                
                response = self.client.chat.completions.create(
                    model="llama-3.1-8b-instant",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=250,
                    top_p=0.9
                )
                
                answer = response.choices[0].message.content.strip()
                return answer
            
            except Exception as e:
                print(f"LLM error: {e}, using fallback method")
        
        # Fallback: Extractive method (query-aware)
        sentences = re.split(r'(?<=[.!?])\s+', context)
        if len(sentences) == 0:
            return "Maaf, tidak dapat memproses konteks dokumen."
        
        query_tokens = set(self.preprocessor.preprocess(query))
        if not query_tokens:
            return ". ".join(sentences[:3]) + "."
        
        # Score sentences by relevance to query
        scored_sentences = []
        for sent in sentences:
            if len(sent.strip()) < 20:
                continue
            sent_tokens = set(self.preprocessor.preprocess(sent))
            overlap = len(query_tokens & sent_tokens)
            jaccard = overlap / len(query_tokens | sent_tokens) if len(query_tokens | sent_tokens) > 0 else 0
            scored_sentences.append((sent, jaccard, overlap))
        
        scored_sentences.sort(key=lambda x: (x[2], x[1]), reverse=True)
        
        # Take top 3-4 most relevant sentences
        n_sentences = min(4, len(scored_sentences))
        top_sentences = [s[0] for i, s in enumerate(scored_sentences[:n_sentences]) if s[2] > 0]
        
        if not top_sentences:
            top_sentences = [s for s in sentences[:4] if len(s.strip()) > 20]
        
        answer = " ".join(top_sentences)
        if answer and not answer.endswith(('.', '!', '?')):
            answer += '.'
        
        return answer if answer else context[:500] + "..."

# ================================================================================
# LOAD DATA & INITIALIZE MODELS
# ================================================================================
@st.cache_resource
def load_system():
    """Load all components once and cache them"""
    try:
        # Load main dataset
        df = pd.read_csv('classified_documents.csv')
        
        # Initialize components
        preprocessor = TextPreprocessor()
        tfidf = TFIDFFromScratch()
        
        # Preprocess documents
        processed_docs = [preprocessor.preprocess(str(text)) for text in df['text']]
        tfidf.fit(processed_docs)
        X_matrix = tfidf.transform(processed_docs)
        
        # Initialize sentiment analyzer
        sentiment_tool = SentimentAnalyzer(use_ml=True)
        
        # Try to load and train sentiment model
        try:
            df_sentiment = pd.read_csv('sentiment_opinions.csv')
            if sentiment_tool.train_from_dataset(df_sentiment):
                sentiment_status = f"K-NN Sentiment (trained on {len(df_sentiment)} samples)"
            else:
                sentiment_status = "Lexicon-based Sentiment (fallback)"
        except FileNotFoundError:
            sentiment_status = "Lexicon-based Sentiment (sentiment_opinions.csv not found)"
        except Exception as e:
            sentiment_status = f"Lexicon-based Sentiment (error: {str(e)[:50]})"
        
        # Initialize answer generator with LLM
        answer_generator = LLMAnswerGenerator()
        
        # Check LLM status
        if answer_generator.use_llm:
            llm_status = "Groq LLM Active"
        else:
            llm_status = "Extractive Mode (Groq API key not found)"
        
        return df, preprocessor, tfidf, X_matrix, sentiment_tool, answer_generator, None, sentiment_status, llm_status
    
    except Exception as e:
        st.error(f"Error loading system: {str(e)}")
        return None, None, None, None, None, None, str(e), "Error", "Error"

# ================================================================================
# MAIN CHATBOT FUNCTION
# ================================================================================
def invest_bot_response(query, df, preprocessor, tfidf, X_matrix, sentiment_tool, answer_generator):
    """Generate response for user query"""
    
    if not query.strip():
        return "Silakan masukkan pertanyaan Anda.", "Netral", "sentiment-neutral", "-", "-"
    
    try:
        # 1. Preprocess query
        q_tokens = preprocessor.preprocess(query)
        if not q_tokens:
            return "Maaf, tidak dapat memproses pertanyaan. Coba gunakan kata kunci yang lebih spesifik.", "Netral", "sentiment-neutral", "-", "-"
        
        q_vec = tfidf.transform([q_tokens])[0]
        
        # 2. Retrieval (Find most similar document)
        sims = [cosine_similarity(q_vec, d_vec) for d_vec in X_matrix]
        top_idx = np.argmax(sims)
        similarity_score = sims[top_idx]
        
        # Check if similarity too low
        if similarity_score < 0.1:
            return ("Maaf, saya tidak menemukan informasi yang cukup relevan dalam database. "
                   "Coba tanyakan tentang saham, crypto, emas, reksadana, atau properti."), "Netral", "sentiment-neutral", "-", "0.0"
        
        context = str(df.iloc[top_idx]['text'])
        source = str(df.iloc[top_idx]['category'])
        doc_id = str(df.iloc[top_idx]['doc_id'])
        
        # 3. Sentiment Analysis
        sentiment, sentiment_class = sentiment_tool.analyze(query)
        
        # 4. Generate Answer (LLM or extractive)
        answer = answer_generator.generate_answer(context, query, source)
        
        return answer, sentiment, sentiment_class, f"{source} (Doc ID: {doc_id})", f"{similarity_score:.2f}"
    
    except Exception as e:
        return f"Terjadi kesalahan: {str(e)}", "Netral", "sentiment-neutral", "-", "-"

# ================================================================================
# STREAMLIT UI
# ================================================================================
def main():
    # Header
    st.markdown('<div class="main-header">InvestBot AI</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Asisten Cerdas Edukasi Investasi - RAG System dengan AI Generator</div>', unsafe_allow_html=True)
    
    # Load system
    with st.spinner('Memuat sistem... Mohon tunggu'):
        result = load_system()
        if len(result) == 9:
            df, preprocessor, tfidf, X_matrix, sentiment_tool, answer_generator, error, sentiment_status, llm_status = result
        else:
            # Fallback jika ada error
            st.error("Gagal memuat sistem")
            return
    
    if error:
        st.error(f"Gagal memuat sistem: {error}")
        st.info("Pastikan file 'classified_documents.csv' ada di direktori yang sama dengan app.py")
        return
    
    st.success(f"Sistem siap! • {sentiment_status} • {llm_status}")
    
    # Sidebar
    with st.sidebar:
        st.header("Informasi")
        st.markdown(f"""
        **Dataset:** {len(df)} dokumen  
        **Kategori:**
        - Saham
        - Cryptocurrency
        - Emas
        - Reksa Dana
        - Obligasi
        
        **Fitur:**
        - RAG System (TF-IDF)
        - K-NN Sentiment Classifier
        - Smart Retrieval
        - AI Answer Generation
        
        **Models:**
        - {sentiment_status}
        - {llm_status}
        """)
        
        st.markdown("---")
        st.markdown("**Contoh Pertanyaan:**")
        
        examples = [
            "Apa itu investasi saham?",
            "Bagaimana cara investasi crypto?",
            "Apa keuntungan investasi emas?",
            "Tips diversifikasi portofolio",
            "Risiko investasi properti"
        ]
        
        for ex in examples:
            if st.button(ex, key=ex):
                st.session_state.query_input = ex
        
        st.markdown("---")
        st.caption("**Mahasiswa:**")
        st.caption("Angelica Widyastuti Kolo")
        st.caption("A11.2021.13212")
        
        st.markdown("---")
        if not answer_generator.use_llm:
            st.info("**Tips:** Tambahkan Groq API key di Streamlit Secrets untuk jawaban lebih natural!")
    
    # Main Content
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("Pertanyaan Anda")
        query = st.text_area(
            "Masukkan pertanyaan tentang investasi:",
            height=150,
            placeholder="Contoh: Bagaimana cara investasi saham untuk pemula?",
            key="query_input"
        )
        
        col_btn1, col_btn2 = st.columns([3, 1])
        with col_btn1:
            submit = st.button("Kirim Pertanyaan", type="primary", use_container_width=True)
        with col_btn2:
            if st.button("Hapus", use_container_width=True):
                st.session_state.query_input = ""
                st.rerun()
    
    with col2:
        st.subheader("Jawaban")
        
        if submit and query:
            with st.spinner('Sedang menganalisis pertanyaan Anda...'):
                answer, sentiment, sent_class, source, similarity = invest_bot_response(
                    query, df, preprocessor, tfidf, X_matrix, sentiment_tool, answer_generator
                )
            
            # Display answer
            st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
            
            # Display metadata
            st.markdown("---")
            col_meta1, col_meta2, col_meta3 = st.columns(3)
            
            with col_meta1:
                st.markdown(f"**Sentimen:**")
                st.markdown(f'<span class="{sent_class}">{sentiment}</span>', unsafe_allow_html=True)
            
            with col_meta2:
                st.markdown(f"**Sumber:**")
                st.text(source)
            
            with col_meta3:
                st.markdown(f"**Relevansi:**")
                st.text(similarity)
        
        elif submit and not query:
            st.warning("Silakan masukkan pertanyaan terlebih dahulu.")
        else:
            st.info("Masukkan pertanyaan dan klik tombol Kirim untuk mendapatkan jawaban.")
    
    # Footer
    st.markdown('<div class="footer">UAS Sistem Temu Kembali Informasi 2025/2026 • Universitas Dian Nuswantoro</div>', 
                unsafe_allow_html=True)

if __name__ == "__main__":
    main()
